{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51835e0c",
   "metadata": {},
   "source": [
    "# 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bb062",
   "metadata": {},
   "source": [
    "## 1. 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed08d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34065dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = os.getenv('HOME') + '/aiffel/transformer_chatbot/Chatbot_data/ChatbotData.csv'\n",
    "dataset = pd.read_csv(csv_path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e9bc4",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a06be",
   "metadata": {},
   "source": [
    "아래 조건을 만족하는 preprocess_sentence() 함수를 구현\n",
    "\n",
    "영문자의 경우, 모두 소문자로 변환\n",
    "\n",
    "영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ccf869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Num: 11,823\n"
     ]
    }
   ],
   "source": [
    "dataset.drop_duplicates(inplace=True)\n",
    "print(f\"Data Num: {len(dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5719dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣0-9?.!,]+\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b8319f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11500</th>\n",
       "      <td>짝녀가 다른반인데 친해지는 방법없을까?</td>\n",
       "      <td>다른 반 친구를 사겨보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11730</th>\n",
       "      <td>커플여행 어떻게 생각해?</td>\n",
       "      <td>누구랑 가느냐가 중요하겠죠.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10046</th>\n",
       "      <td>생각이 자꾸 나</td>\n",
       "      <td>좋아하나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10179</th>\n",
       "      <td>썸 타는 거 티내고 싶진 않아.</td>\n",
       "      <td>누구에게요?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8035</th>\n",
       "      <td>정말 모든게 그립지만.</td>\n",
       "      <td>묻어두는 것도 좋겠지요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Q                A  label\n",
       "11500  짝녀가 다른반인데 친해지는 방법없을까?  다른 반 친구를 사겨보세요.      2\n",
       "11730          커플여행 어떻게 생각해?  누구랑 가느냐가 중요하겠죠.      2\n",
       "10046               생각이 자꾸 나          좋아하나봐요.      2\n",
       "10179      썸 타는 거 티내고 싶진 않아.           누구에게요?      2\n",
       "8035            정말 모든게 그립지만.    묻어두는 것도 좋겠지요.      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"Q\"] = dataset[\"Q\"].apply(preprocess_sentence)\n",
    "dataset[\"A\"] = dataset[\"A\"].apply(preprocess_sentence)\n",
    "\n",
    "display(dataset.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca0486",
   "metadata": {},
   "source": [
    "## 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a80e13",
   "metadata": {},
   "source": [
    "토큰화에는 KoNLPy의 mecab 클래스를 사용\n",
    "\n",
    "아래 조건을 만족하는 build_corpus() 함수를 구현\n",
    "\n",
    "* 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받는다.\n",
    "\n",
    "* 데이터를 앞서 정의한 preprocess_sentence() 함수로 정제하고, 토큰화한다.\n",
    "\n",
    "* 토큰화는 전달받은 토크나이즈 함수를 사용합니다. 이번엔 mecab.morphs 함수를 전달\n",
    "\n",
    "* 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외\n",
    "\n",
    "* 중복되는 문장은 데이터에서 제외합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17e12b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4925bcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>발목 접질렀어</td>\n",
       "      <td>&lt;sos&gt; 꾸준히 치료하세요. &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11380</th>\n",
       "      <td>짝남이 나한테 관심 없어보여도 계속 연락하고 들이대?</td>\n",
       "      <td>&lt;sos&gt; 확실한 거절이 아니라면 부담스럽지 않은 선에서 연락하는게 좋겠어요. &lt;eos&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>오랜만에 아침 먹었어</td>\n",
       "      <td>&lt;sos&gt; 좋은 식습관이에요. &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>비밀번호 자꾸 바꿔</td>\n",
       "      <td>&lt;sos&gt; 보안상 그게 좋죠. &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10543</th>\n",
       "      <td>여자들이 보통 좋아하는 음식이 뭐야?</td>\n",
       "      <td>&lt;sos&gt; 사람 마다 다르겠지요. &lt;eos&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Q  \\\n",
       "1946                         발목 접질렀어   \n",
       "11380  짝남이 나한테 관심 없어보여도 계속 연락하고 들이대?   \n",
       "3372                     오랜만에 아침 먹었어   \n",
       "2210                      비밀번호 자꾸 바꿔   \n",
       "10543           여자들이 보통 좋아하는 음식이 뭐야?   \n",
       "\n",
       "                                                       A  label  \n",
       "1946                              <sos> 꾸준히 치료하세요. <eos>      0  \n",
       "11380  <sos> 확실한 거절이 아니라면 부담스럽지 않은 선에서 연락하는게 좋겠어요. <eos>      2  \n",
       "3372                              <sos> 좋은 식습관이에요. <eos>      0  \n",
       "2210                              <sos> 보안상 그게 좋죠. <eos>      0  \n",
       "10543                           <sos> 사람 마다 다르겠지요. <eos>      2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"A\"] = dataset[\"A\"].apply(lambda x: \"<sos> \" + x + \" <eos>\")\n",
    "\n",
    "display(dataset.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f23580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Vocab Size: 21,745\n"
     ]
    }
   ],
   "source": [
    "def get_tokenizer(corpus, vocab_size):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters='',\n",
    "        oov_token=\"<UNK>\",\n",
    "        num_words=vocab_size\n",
    "    )\n",
    "    corpus_input = [sentence.split() for sentence in corpus]\n",
    "    tokenizer.fit_on_texts(corpus_input)\n",
    "    \n",
    "    if vocab_size is not None:\n",
    "        words_frequency = [w for w,c in tokenizer.word_index.items() if c >= vocab_size + 1]\n",
    "        for w in words_frequency:\n",
    "            del tokenizer.word_index[w]\n",
    "            del tokenizer.word_counts[w]\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "concat = pd.concat([dataset[\"Q\"], dataset[\"A\"]])\n",
    "tokenizer = get_tokenizer(concat, None)\n",
    "\n",
    "print(\"Tokenizer Vocab Size:\", f\"{len(tokenizer.word_index):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e48bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_sentence(copus, tokenizer):\n",
    "    tensor = tokenizer.texts_to_sequences(copus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tensor, padding='post'\n",
    "    )\n",
    "    return tensor\n",
    "\n",
    "\n",
    "enc_tensor = encoding_sentence(dataset[\"Q\"], tokenizer)\n",
    "dec_tensor = encoding_sentence(dataset[\"A\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cda7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordNumByFreq(tokenizer, freq_num):\n",
    "    sorted_freq = sorted(tokenizer.word_counts.items(), key=lambda x: x[1])\n",
    "    for idx, (_, freq) in enumerate(sorted_freq):\n",
    "        if freq > freq_num: break;\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484cd9e",
   "metadata": {},
   "source": [
    "질문(Question) 문장의 길이가 15 이하이고 대답(Answer) 문장의 길이가 18 이하인 문장만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2e0021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Vocab Size: 5,872\n"
     ]
    }
   ],
   "source": [
    "concat = pd.concat([dataset[\"Q\"], dataset[\"A\"]])\n",
    "tokenizer = get_tokenizer(concat, 5872)\n",
    "\n",
    "q = dataset[\"Q\"].apply(lambda x: len(tokenizer.texts_to_sequences([x])[0]) <= 15)\n",
    "a = dataset[\"A\"].apply(lambda x: len(tokenizer.texts_to_sequences([x])[0]) <= 18)\n",
    "dataset = dataset[q & a]\n",
    "\n",
    "print(\"Tokenizer Vocab Size:\", f\"{len(tokenizer.word_index):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fcfa2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sns 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>&lt;sos&gt; 시간을 정하고 해보세요. &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>&lt;sos&gt; 여행은 언제나 좋죠. &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>개강룩 입어볼까</td>\n",
       "      <td>&lt;sos&gt; 개시해보세요. &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>가족관계 알려 줘</td>\n",
       "      <td>&lt;sos&gt; 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>가족이 누구야?</td>\n",
       "      <td>&lt;sos&gt; 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Q                                                  A  \\\n",
       "9   sns 시간낭비인데 자꾸 보게됨                          <sos> 시간을 정하고 해보세요. <eos>   \n",
       "3     3박4일 정도 놀러가고 싶다                            <sos> 여행은 언제나 좋죠. <eos>   \n",
       "80           개강룩 입어볼까                                <sos> 개시해보세요. <eos>   \n",
       "27          가족관계 알려 줘  <sos> 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하...   \n",
       "35           가족이 누구야?  <sos> 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하...   \n",
       "\n",
       "    label  \n",
       "9       0  \n",
       "3       0  \n",
       "80      0  \n",
       "27      0  \n",
       "35      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = dataset[:100]\n",
    "dataset = dataset[100:]\n",
    "\n",
    "display(test_dataset.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462efc6",
   "metadata": {},
   "source": [
    "## 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aba360",
   "metadata": {},
   "source": [
    "pip install --upgrade gensim==3.8.3\n",
    "\n",
    "위의 실행문을 터미널에 입력하여 gensim의 버전을 다운그레이드 해줘서 ko.bin 파일 로드하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a4c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = os.getenv('HOME') + '/aiffel/transformer_chatbot/Chatbot_data/ko.bin'\n",
    "w2v = gensim.models.Word2Vec.load(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ddb8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec, enc_arg=True):\n",
    "    toks = sentence.split()\n",
    "    if not enc_arg:   #<sos>, <eos> 토큰 제외\n",
    "        toks = toks[1:-1]\n",
    "\n",
    "    _from = random.choice(toks)\n",
    "    \n",
    "    try:\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "    except:\n",
    "        return \"_\"\n",
    "    \n",
    "    res = \"\"\n",
    "    for tok in sentence.split():\n",
    "        if tok == _from:\n",
    "            res += _to + \" \"\n",
    "        else:\n",
    "            res += tok + \" \"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b367f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argument_data(dataset, word2vec, enc_arg=True):\n",
    "    qna = \"Q\" if enc_arg else \"A\"\n",
    "    arg = dataset[qna].apply(lambda x: lexical_sub(x, word2vec, enc_arg))\n",
    "    \n",
    "    arg_data = dataset.copy()\n",
    "    arg_data[qna] = arg\n",
    "    \n",
    "    arg_data = arg_data[arg_data[qna] != \"_\"]\n",
    "    return arg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57070444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_342/428797383.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  _to = word2vec.most_similar(_from)[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Sentence: 벚꽃이 너무 예뻐 ======> 벚꽃이 워낙 예뻐 \n",
      "Answer Sentence: <sos> 너무 아름답죠. <eos> ======> <sos> 워낙 아름답죠. <eos> \n"
     ]
    }
   ],
   "source": [
    "enc_alpha = argument_data(dataset, w2v, True)\n",
    "dec_alpha = argument_data(dataset, w2v, False)\n",
    "\n",
    "\n",
    "\n",
    "enc_idx = set(dataset.index)\n",
    "enc_alpha_idx = set(enc_alpha.index)\n",
    "dec_alpha_idx = set(dec_alpha.index)\n",
    "\n",
    "vet = enc_idx & enc_alpha_idx & dec_alpha_idx\n",
    "vet = list(vet)[0]\n",
    "\n",
    "print(f\"Question Sentence: {dataset['Q'][vet]} ======> {enc_alpha['Q'][vet]}\")\n",
    "print(f\"Answer Sentence: {dataset['A'][vet]} ======> {dec_alpha['A'][vet]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50d3490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Num: 17,423\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4296</th>\n",
       "      <td>지하철에 사람이 너무 많아</td>\n",
       "      <td>&lt;sos&gt; 맨 앞이나 맨 뒤에 타세요. &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>1년이 아무것도 아니였나</td>\n",
       "      <td>&lt;sos&gt; 아무것도 아닌 건 아닐 거예요. &lt;eos&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3553</th>\n",
       "      <td>운명인가</td>\n",
       "      <td>&lt;sos&gt; 인연인가 봐요. &lt;eos&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>썸 타는 사람 있는데 망설여지는데</td>\n",
       "      <td>&lt;sos&gt; 어떤 부분이 망설여지는지 말씀해보세요. &lt;eos&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11528</th>\n",
       "      <td>짝녀랑 드디어 사귄다!</td>\n",
       "      <td>&lt;sos&gt; 좋은 소식이네요. &lt;eos&gt;</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Q                                  A  label\n",
       "4296       지하철에 사람이 너무 많아        <sos> 맨 앞이나 맨 뒤에 타세요. <eos>      0\n",
       "5310        1년이 아무것도 아니였나      <sos> 아무것도 아닌 건 아닐 거예요. <eos>      1\n",
       "3553                 운명인가               <sos> 인연인가 봐요. <eos>      0\n",
       "10195  썸 타는 사람 있는데 망설여지는데  <sos> 어떤 부분이 망설여지는지 말씀해보세요. <eos>      2\n",
       "11528        짝녀랑 드디어 사귄다!              <sos> 좋은 소식이네요. <eos>      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.concat([dataset, enc_alpha, dec_alpha])\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "print(f\"Dataset Num: {len(dataset):,}\")\n",
    "display(dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2ef1ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data num: 17,423\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = encoding_sentence(dataset[\"Q\"], tokenizer)\n",
    "dec_tensor = encoding_sentence(dataset[\"A\"], tokenizer)\n",
    "\n",
    "print(\"Data num:\", f\"{len(enc_tensor):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2ad53",
   "metadata": {},
   "source": [
    "## 5. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28022e",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "933a6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i)/d_model)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    \n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b596124",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "043e0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29d5a8",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6cfb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf095cc",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b0eb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f915e8",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "908945c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # Masked Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        #out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        #out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3d4ca",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51761c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "            \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313edace",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a62708b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        \n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7a829",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11b17348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size,\n",
    "                 pos_len, dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        # 1. Embedding Layer 정의\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding 정의\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        # 6. Dropout 정의\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        # 3. Encoder / Decoder 정의\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        # 4. Output Linear 정의\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        # 5. Shared Weights\n",
    "        self.shared = shared\n",
    "        \n",
    "        if shared:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "        \n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding\n",
    "        + Shared일 경우 Scaling 작업 포함\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        \n",
    "        if self.shared:\n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "        \n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "        # Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        # Step 3: Decoder(dec_in, enc_out, mask) -> dec_out, dec_attns, dec_enc_attns\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        # Step 4: Out Linear(dec_out) -> logits\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5418a6",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5a3885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e43a3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c3dbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b6bb9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26ca09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7595413",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    d_ff=256,\n",
    "    src_vocab_size=5872,\n",
    "    tgt_vocab_size=5872,\n",
    "    pos_len=200,\n",
    "    dropout=0.5,\n",
    "    shared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78f9d943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 273/273 [00:10<00:00, 26.31it/s, Loss 6.9641]\n",
      "Epoch  2: 100%|██████████| 273/273 [00:03<00:00, 73.41it/s, Loss 5.2730]\n",
      "Epoch  3: 100%|██████████| 273/273 [00:03<00:00, 72.29it/s, Loss 4.8571]\n",
      "Epoch  4: 100%|██████████| 273/273 [00:03<00:00, 73.17it/s, Loss 4.4409]\n",
      "Epoch  5: 100%|██████████| 273/273 [00:03<00:00, 73.17it/s, Loss 3.9131]\n",
      "Epoch  6: 100%|██████████| 273/273 [00:03<00:00, 71.02it/s, Loss 3.2623]\n",
      "Epoch  7: 100%|██████████| 273/273 [00:03<00:00, 72.57it/s, Loss 2.6414]\n",
      "Epoch  8: 100%|██████████| 273/273 [00:03<00:00, 72.87it/s, Loss 2.1339]\n",
      "Epoch  9: 100%|██████████| 273/273 [00:03<00:00, 71.11it/s, Loss 1.7424]\n",
      "Epoch 10: 100%|██████████| 273/273 [00:03<00:00, 72.06it/s, Loss 1.4559]\n"
     ]
    }
   ],
   "source": [
    "def model_fit(enc_train, dec_train, model, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        idx_list = list(range(0, enc_train.shape[0], batch_size))\n",
    "        random.shuffle(idx_list)\n",
    "        t = tqdm(idx_list)\n",
    "\n",
    "        for (batch, idx) in enumerate(t):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            train_step(\n",
    "                enc_train[idx:idx+batch_size],\n",
    "                dec_train[idx:idx+batch_size],\n",
    "                model,\n",
    "                optimizer\n",
    "            )\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "            \n",
    "\n",
    "model_fit(enc_tensor, dec_tensor, transformer, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96692a3",
   "metadata": {},
   "source": [
    "## 6. 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e483af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Quenstion: 지루하다, 놀러가고 싶어.                \tAnswer: <UNK> 많이 <UNK>                \n",
      "Quenstion: 오늘 일찍 일어났더니 피곤하다.             \tAnswer: <UNK> 시간이 <UNK>               \n",
      "Quenstion: 간만에 여자친구랑 데이트 하기로 했어.         \tAnswer: <UNK> <UNK>                   \n",
      "Quenstion: 집에 있는다는 소리야.                  \tAnswer: <UNK> 싶지 않아요.                 \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence, model, tokenizer, enc_tensor, dec_tensor):\n",
    "    enc_maxlen = enc_tensor.shape[-1]\n",
    "    dec_maxlen = dec_tensor.shape[-1]\n",
    "\n",
    "    sos_idx = tokenizer.word_index['<sos>']\n",
    "    eos_idx = tokenizer.word_index['<eos>']\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    m = Mecab()\n",
    "    sentence = m.morphs(sentence)\n",
    "\n",
    "    _input = tokenizer.texts_to_sequences([sentence])\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        _input,\n",
    "        maxlen=enc_maxlen,\n",
    "        padding='post'\n",
    "    )\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([sos_idx], 0)\n",
    "\n",
    "    for i in range(dec_maxlen):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(\n",
    "            _input, output\n",
    "        )\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(\n",
    "            _input, output, enc_padding_mask, combined_mask, dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predicted_id = tf.argmax(\n",
    "            tf.math.softmax(predictions, axis=-1)[0, -1]\n",
    "        ).numpy().item()\n",
    "\n",
    "        if predicted_id == eos_idx:\n",
    "            result = tokenizer.sequences_to_texts([ids])\n",
    "            return result\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = tokenizer.sequences_to_texts([ids])\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"=\" * 100)\n",
    "test_sentences = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    ans = translate(sentence, transformer, tokenizer, enc_tensor, dec_tensor)[0]\n",
    "    print(f\"Quenstion: {sentence:<30}\\tAnswer: {ans:<30}\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b1993",
   "metadata": {},
   "source": [
    "## 7. 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d62a01e",
   "metadata": {},
   "source": [
    "굉장히 어려운 노드였다.\n",
    "\n",
    "선배님들의 코드를 참고하면서 했는데도 결과가 잘 나오지 않았다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
