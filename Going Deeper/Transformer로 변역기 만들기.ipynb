{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer로 변역기 만들기\n",
    "\n",
    "혁신적이었던 Seq2seq 구조로 번역기를 만들었으나 성능이 기대에 미치지 않았다.\n",
    "\n",
    "Trasformer는 현재까지도 각종 번역 부문에서 최고의 성능을 자랑하는 모델이니 이번에는 정말 멋진 번역기를 만들 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME') + '/aiffel/transformer/data/korean-english-park.train/'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)  # kor, eng가 같은 갯수라는 것을 검증받기 위해 적용\n",
    "    \n",
    "    cleaned_corpus = list(set(zip(kor, eng)))  # 중복된 데이터 제거\n",
    "    \n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 모든 입력을 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    # 알파벳, 문장부호, 한글만 남기고 모두 제거\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,]+\", \" \", sentence)\n",
    "    # 문장부호 양옆에 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    # 문장 앞뒤의 불필요한 공백 제거\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성\n",
    "def generate_tokenizer(corpus, vocab_size, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "    # corpus를 받아 txt파일로 저장\n",
    "    temp_file = os.getenv('HOME') + f'/aiffel/transformer/data/corpus_{lang}.txt'\n",
    "    \n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    \n",
    "    # Sentencepiece를 이용해 \n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={temp_file} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} \\\n",
    "        --unk_id={unk_id} --model_prefix=spm_{lang} --vocab_size={vocab_size}'\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'spm_{lang}.model')\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('젠 프사키 오바마측 선거운동 대변인은 젠 던햄이 9일 밤 자택에서 평화롭게 잠들었다고 전했다.',\n",
       " 'Dunham passed away peacefully at her home shortly before midnight Sunday night (5 a.m. ET), campaign spokeswoman Jen Psaki told CNN.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair[0], pair[1]\n",
    "    # kor, eng 나눠서 데이터 정제 후 분리\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78968/78968 [00:03<00:00, 21282.08it/s]\n"
     ]
    }
   ],
   "source": [
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남김\n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    src = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    tgt = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "    \n",
    "    if len(src) <= 50 and len(tgt) <= 50:\n",
    "        src_corpus.append(src)\n",
    "        tgt_corpus.append(tgt)\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos - 단어가 위치한 Time-step(각각의 토큰의 위치정보값이며 정수값을 의미)\n",
    "# d_model - 모델의 Embedding 차원 수\n",
    "# i - Encoding차원의 index\n",
    "\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i)/d_model)  # np.power(a,b) > a^b(제곱)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    \n",
    "    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)  # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        \n",
    "        # Scaled QK 값 구하기\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)\n",
    "        \n",
    "        # 1. Attention Weights 값 구하기 -> attentions\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        # 2. Attention 값을 V에 곱하기 -> out\n",
    "        out = tf.matmul(attentions, V)\n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding된 입력을 head의 수로 분할하는 함수\n",
    "        \n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x length x heads x self.depth ]\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "        return split_x\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "        \n",
    "        x: [ batch x length x heads x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "        \"\"\"\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # Masked Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        #out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        #out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "            \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-7. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        \n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-8. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size,\n",
    "                 pos_len, dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        # 1. Embedding Layer 정의\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding 정의\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        # 6. Dropout 정의\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        # 3. Encoder / Decoder 정의\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        # 4. Output Linear 정의\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        # 5. Shared Weights\n",
    "        self.shared = shared\n",
    "        \n",
    "        if shared:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "        \n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding\n",
    "        + Shared일 경우 Scaling 작업 포함\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        \n",
    "        if self.shared:\n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "        \n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "        \n",
    "        # Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        # Step 3: Decoder(dec_in, enc_out, mask) -> dec_out, dec_attns, dec_enc_attns\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        # Step 4: Out Linear(dec_out) -> logits\n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-9. Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=enc_train.shape[-1], padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "        # InvalidArgumentError: In[0] mismatch In[1] shape: 50 vs. 1: [1,8,1,50] [1,8,1,64] 0 0 [Op:BatchMatMulV2]\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.2,\n",
    "    shared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 2127/2127 [04:56<00:00,  7.16it/s, Loss 5.1270]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama has a presidential candidate in the presidential race .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they are also also also between the two countries .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it was not clear .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2:   0%|          | 1/2127 [00:00<04:55,  7.19it/s, Loss 3.9358]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the deaths of deaths were killed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 2127/2127 [04:52<00:00,  7.28it/s, Loss 3.9443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: it is the first time in the president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is in the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee coffee , coffees , coffee , coffee  coffee , coffee  coffee  coffees .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3:   0%|          | 1/2127 [00:00<05:13,  6.78it/s, Loss 3.2751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: on thursday , the death toll has killed more than people .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 2127/2127 [04:52<00:00,  7.27it/s, Loss 3.4169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president elect barack obama .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is the national mountain .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: no need to be needed to be no needed to be needed to be no needed to be needed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4:   0%|          | 1/2127 [00:00<05:05,  6.97it/s, Loss 1.9796]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: the death toll from the death toll in the latest incident .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 2127/2127 [04:52<00:00,  7.27it/s, Loss 2.8142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is a great president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city s city is getting the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: even greener .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5:   0%|          | 1/2127 [00:00<04:53,  7.25it/s, Loss 2.2579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: nine people were killed and injured .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 2127/2127 [04:52<00:00,  7.27it/s, Loss 2.2674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is in town .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: coffee has no need for coffee .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6:   0%|          | 1/2127 [00:00<04:59,  7.11it/s, Loss 1.7310]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were dead and injured thursday .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 2127/2127 [04:52<00:00,  7.27it/s, Loss 1.7896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is s advantage .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they are in the city of lives in the city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there is no need for coffee .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7:   0%|          | 1/2127 [00:00<05:01,  7.05it/s, Loss 1.1205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were dead and injured .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 2127/2127 [04:52<00:00,  7.26it/s, Loss 1.3776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: i think barack obama is a president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city mayor is in city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: nothing need to worry about needs .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8:   0%|          | 1/2127 [00:00<05:04,  6.98it/s, Loss 0.8879]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were dead and seven others were injured last sunday .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 2127/2127 [04:52<00:00,  7.26it/s, Loss 1.0501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president barack obama is .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city until there is loved city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s no need for coffee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9:   0%|          | 1/2127 [00:00<04:45,  7.44it/s, Loss 0.8751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven were involved in the deadly clashes .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 2127/2127 [04:53<00:00,  7.26it/s, Loss 0.8154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the democratic nominee .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city is in city of city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s needed to show if you need .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 1/2127 [00:00<04:50,  7.32it/s, Loss 0.5552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seventeen people were dead , and seven were older .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2127/2127 [04:53<00:00,  7.26it/s, Loss 0.6605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president obama is .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city latest city visit the city\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there is no need for coffee .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 1/2127 [00:00<05:04,  6.98it/s, Loss 0.2362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven were killed sunday high .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 2127/2127 [04:53<00:00,  7.26it/s, Loss 0.5599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is what is .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city late mark the city is love .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there need nothing needs to have to worry .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12:   0%|          | 1/2127 [00:00<04:54,  7.23it/s, Loss 0.1847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven of the dead were wounded .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 2127/2127 [04:53<00:00,  7.25it/s, Loss 0.4912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is what president is his soleway man .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city latestly in the city visit\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: nothing need is needed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13:   0%|          | 1/2127 [00:00<04:59,  7.11it/s, Loss 0.4215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: incidents occurred wednesday .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 2127/2127 [04:53<00:00,  7.24it/s, Loss 0.4401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is aides .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city mayor is city of san francisco .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s no coffee need for coffee .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14:   0%|          | 1/2127 [00:00<04:59,  7.11it/s, Loss 0.2103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven were carried by senior officials wednesday .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 2127/2127 [04:53<00:00,  7.25it/s, Loss 0.4019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city lately three urban drives in city .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2127 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s no need for coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were dead .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 2127/2127 [04:55<00:00,  7.19it/s, Loss 0.3731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is what president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city surpasss the city mayor in city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s no coffee need for coffee .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16:   0%|          | 1/2127 [00:00<04:47,  7.40it/s, Loss 0.2600]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed and seven others .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 2127/2127 [04:56<00:00,  7.18it/s, Loss 0.3472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is what is a doctor .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: some city searching for the urban r .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2127 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s nothing to drink coffee .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were dead .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 2127/2127 [04:56<00:00,  7.18it/s, Loss 0.3290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the democratic president .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city feels in the city is city .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s a coffee need for coffee .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18:   0%|          | 1/2127 [00:00<05:01,  7.04it/s, Loss 0.0926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 2127/2127 [04:56<00:00,  7.18it/s, Loss 0.3105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president obama is .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city officials in sanford .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: it s needed .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19:   0%|          | 1/2127 [00:00<05:03,  7.01it/s, Loss 0.0939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed and seven others .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2127/2127 [04:55<00:00,  7.21it/s, Loss 0.3010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama is the democratic nominee .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the city captured the san residence of san francisco .\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s a coffee need for coffee .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20:   0%|          | 1/2127 [00:00<05:01,  7.05it/s, Loss 0.1223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were confirmed dead .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 2127/2127 [04:53<00:00,  7.24it/s, Loss 0.2875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president barack obama is the last thing .\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: city last city . city i discover the city\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there s a coffeer .\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: seven people were killed by another person .\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                                                     dec_train[idx:idx+BATCH_SIZE],\n",
    "                                                                     transformer,\n",
    "                                                                     optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    # 매 Epoch 마다 제시된 예문에 대한 번역 생성\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결론\n",
    "\n",
    "Loss 0.2875\n",
    "\n",
    "Input: 오바마는 대통령이다.\n",
    "\n",
    "Predicted translation: president barack obama is the last thing .\n",
    "\n",
    "Input: 시민들은 도시 속에 산다.\n",
    "\n",
    "Predicted translation: city last city . city i discover the city\n",
    "\n",
    "Input: 커피는 필요 없다.\n",
    "\n",
    "Predicted translation: there s a coffeer .\n",
    "\n",
    "Input: 일곱 명의 사망자가 발생했다.\n",
    "\n",
    "Predicted translation: seven people were killed by another person ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2seq 구조를 이용한 번역기보다 성능이 향상되어 핵심 단어는 구현할 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고\n",
    "* 학습시간이 너무 오래 걸렸고, 많은 코드를 직접 구현 해야해서 어려웠던 프로젝트였다.\n",
    "* 그래도 논문 구현을 했던 경험을 바탕으로 해서 이해하고 완성을 한 것에 대해 뿌듯함을 느낀다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
