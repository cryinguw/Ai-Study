{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddad4601",
   "metadata": {},
   "source": [
    "# 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c3484",
   "metadata": {},
   "source": [
    "## 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "265b3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff5cda9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 217975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29994</th>\n",
       "      <td>Whose cup is this?</td>\n",
       "      <td>C’est à qui, cette tasse ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84938</th>\n",
       "      <td>You won't bleed to death.</td>\n",
       "      <td>Tu ne saigneras pas jusqu'à ce que mort s'ensu...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178246</th>\n",
       "      <td>You don't need to think about that now.</td>\n",
       "      <td>Vous n'avez pas besoin d'y penser maintenant.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>I'm lucky.</td>\n",
       "      <td>Je suis veinarde.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126550</th>\n",
       "      <td>Why are they doing this to me?</td>\n",
       "      <td>Pourquoi me font-ils ça ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            eng  \\\n",
       "29994                        Whose cup is this?   \n",
       "84938                 You won't bleed to death.   \n",
       "178246  You don't need to think about that now.   \n",
       "1212                                 I'm lucky.   \n",
       "126550           Why are they doing this to me?   \n",
       "\n",
       "                                                      fra  \\\n",
       "29994                          C’est à qui, cette tasse ?   \n",
       "84938   Tu ne saigneras pas jusqu'à ce que mort s'ensu...   \n",
       "178246      Vous n'avez pas besoin d'y penser maintenant.   \n",
       "1212                                    Je suis veinarde.   \n",
       "126550                          Pourquoi me font-ils ça ?   \n",
       "\n",
       "                                                       cc  \n",
       "29994   CC-BY 2.0 (France) Attribution: tatoeba.org #8...  \n",
       "84938   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "178246  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  \n",
       "1212    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "126550  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022a514f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8050</th>\n",
       "      <td>I'm a dentist.</td>\n",
       "      <td>Je suis dentiste.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31320</th>\n",
       "      <td>Everybody did that.</td>\n",
       "      <td>Tout le monde a fait ça.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28496</th>\n",
       "      <td>They were panting.</td>\n",
       "      <td>Elles tiraient la langue.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27338</th>\n",
       "      <td>It's unauthorized.</td>\n",
       "      <td>Ce n'est pas autorisé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14819</th>\n",
       "      <td>I caught a cold.</td>\n",
       "      <td>J'ai contracté un rhume.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                        fra\n",
       "8050        I'm a dentist.          Je suis dentiste.\n",
       "31320  Everybody did that.   Tout le monde a fait ça.\n",
       "28496   They were panting.  Elles tiraient la langue.\n",
       "27338   It's unauthorized.     Ce n'est pas autorisé.\n",
       "14819     I caught a cold.   J'ai contracté un rhume."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8c8f4",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7fbbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48cd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence_decoder(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6245d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng = lines.eng.apply(lambda x : preprocess_sentence(x))\n",
    "lines.fra = lines.fra.apply(lambda x : preprocess_sentence_decoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d86cef37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22789        [tom, knows, better, .]\n",
       "18752       [can, you, prove, it, ?]\n",
       "32460    [i, have, to, go, there, .]\n",
       "23966       [you, may, go, there, .]\n",
       "855                   [we, agree, .]\n",
       "Name: eng, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.eng.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de64e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(lines.eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb97fa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[27, 1], [27, 1], [27, 1]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c90b707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_tokenizer = Tokenizer()\n",
    "fra_tokenizer.fit_on_texts(lines.fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bc5a1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 72, 9, 2], [1, 340, 3, 2], [1, 27, 523, 9, 2]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fd5d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4514\n",
      "프랑스어 단어장의 크기 : 7263\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ba6bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a605128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4514\n",
      "프랑스어 단어장의 크기 : 7263\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b2246b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = '<start>'\n",
    "eos_token = '<end>'\n",
    "\n",
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bcd0cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07b307e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0566ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 8)\n",
      "(30000, 17)\n",
      "(30000, 17)\n",
      "(3000, 8)\n",
      "(3000, 17)\n",
      "(3000, 17)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print(encoder_input_train.shape)\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586e89e",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c534b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "# encoder embedding\n",
    "enc_emb = Embedding(eng_vocab_size, 256, input_length=max_eng_seq_len)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c4ba752",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "# decoder embedding\n",
    "dec_emb = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(dec_masking, initial_state = encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5a0900b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    1155584     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1859328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 256)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 256)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7263)   1866591     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,932,127\n",
      "Trainable params: 5,932,127\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b66a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c406a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "938/938 [==============================] - 50s 20ms/step - loss: 1.4439 - acc: 0.7739 - val_loss: 1.1834 - val_acc: 0.8138\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 1.0895 - acc: 0.8260 - val_loss: 1.0375 - val_acc: 0.8346\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.9652 - acc: 0.8435 - val_loss: 0.9511 - val_acc: 0.8475\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.8815 - acc: 0.8547 - val_loss: 0.8872 - val_acc: 0.8562\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.8203 - acc: 0.8639 - val_loss: 0.8447 - val_acc: 0.8629\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.7680 - acc: 0.8720 - val_loss: 0.8154 - val_acc: 0.8663\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.7253 - acc: 0.8789 - val_loss: 0.7927 - val_acc: 0.8717\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.6948 - acc: 0.8850 - val_loss: 0.7844 - val_acc: 0.8744\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.6730 - acc: 0.8904 - val_loss: 0.7836 - val_acc: 0.8749\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.6529 - acc: 0.8957 - val_loss: 0.7733 - val_acc: 0.8788\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.6396 - acc: 0.8999 - val_loss: 0.7714 - val_acc: 0.8808\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.6229 - acc: 0.9038 - val_loss: 0.7604 - val_acc: 0.8819\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.6041 - acc: 0.9073 - val_loss: 0.7657 - val_acc: 0.8823\n",
      "Epoch 14/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5972 - acc: 0.9104 - val_loss: 0.7674 - val_acc: 0.8837\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.5903 - acc: 0.9128 - val_loss: 0.7707 - val_acc: 0.8831\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.5802 - acc: 0.9147 - val_loss: 0.7607 - val_acc: 0.8837\n",
      "Epoch 17/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5655 - acc: 0.9173 - val_loss: 0.7624 - val_acc: 0.8843\n",
      "Epoch 18/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5622 - acc: 0.9187 - val_loss: 0.7680 - val_acc: 0.8839\n",
      "Epoch 19/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.5582 - acc: 0.9202 - val_loss: 0.7678 - val_acc: 0.8840\n",
      "Epoch 20/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.5526 - acc: 0.9217 - val_loss: 0.7700 - val_acc: 0.8843\n",
      "Epoch 21/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.5453 - acc: 0.9231 - val_loss: 0.7673 - val_acc: 0.8847\n",
      "Epoch 22/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5376 - acc: 0.9243 - val_loss: 0.7672 - val_acc: 0.8842\n",
      "Epoch 23/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.5294 - acc: 0.9253 - val_loss: 0.7650 - val_acc: 0.8842\n",
      "Epoch 24/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.5232 - acc: 0.9262 - val_loss: 0.7652 - val_acc: 0.8845\n",
      "Epoch 25/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5159 - acc: 0.9273 - val_loss: 0.7594 - val_acc: 0.8856\n",
      "Epoch 26/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.5093 - acc: 0.9283 - val_loss: 0.7588 - val_acc: 0.8857\n",
      "Epoch 27/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.5028 - acc: 0.9292 - val_loss: 0.7592 - val_acc: 0.8856\n",
      "Epoch 28/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4970 - acc: 0.9297 - val_loss: 0.7572 - val_acc: 0.8846\n",
      "Epoch 29/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4915 - acc: 0.9304 - val_loss: 0.7562 - val_acc: 0.8850\n",
      "Epoch 30/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4857 - acc: 0.9309 - val_loss: 0.7581 - val_acc: 0.8851\n",
      "Epoch 31/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.4805 - acc: 0.9319 - val_loss: 0.7531 - val_acc: 0.8855\n",
      "Epoch 32/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4745 - acc: 0.9324 - val_loss: 0.7518 - val_acc: 0.8859\n",
      "Epoch 33/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4701 - acc: 0.9325 - val_loss: 0.7496 - val_acc: 0.8859\n",
      "Epoch 34/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4651 - acc: 0.9334 - val_loss: 0.7521 - val_acc: 0.8862\n",
      "Epoch 35/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4610 - acc: 0.9340 - val_loss: 0.7523 - val_acc: 0.8858\n",
      "Epoch 36/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4565 - acc: 0.9342 - val_loss: 0.7513 - val_acc: 0.8852\n",
      "Epoch 37/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4531 - acc: 0.9345 - val_loss: 0.7529 - val_acc: 0.8847\n",
      "Epoch 38/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.4487 - acc: 0.9351 - val_loss: 0.7518 - val_acc: 0.8855\n",
      "Epoch 39/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4457 - acc: 0.9355 - val_loss: 0.7524 - val_acc: 0.8858\n",
      "Epoch 40/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4425 - acc: 0.9357 - val_loss: 0.7560 - val_acc: 0.8850\n",
      "Epoch 41/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4393 - acc: 0.9362 - val_loss: 0.7570 - val_acc: 0.8841\n",
      "Epoch 42/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4370 - acc: 0.9362 - val_loss: 0.7599 - val_acc: 0.8848\n",
      "Epoch 43/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4340 - acc: 0.9366 - val_loss: 0.7566 - val_acc: 0.8851\n",
      "Epoch 44/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4308 - acc: 0.9369 - val_loss: 0.7600 - val_acc: 0.8851\n",
      "Epoch 45/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.4282 - acc: 0.9373 - val_loss: 0.7572 - val_acc: 0.8854\n",
      "Epoch 46/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4252 - acc: 0.9376 - val_loss: 0.7602 - val_acc: 0.8848\n",
      "Epoch 47/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4227 - acc: 0.9376 - val_loss: 0.7623 - val_acc: 0.8850\n",
      "Epoch 48/50\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 0.4201 - acc: 0.9381 - val_loss: 0.7578 - val_acc: 0.8856\n",
      "Epoch 49/50\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.4180 - acc: 0.9383 - val_loss: 0.7606 - val_acc: 0.8851\n",
      "Epoch 50/50\n",
      "938/938 [==============================] - 16s 17ms/step - loss: 0.4160 - acc: 0.9384 - val_loss: 0.7626 - val_acc: 0.8848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76ea062760>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_train, decoder_input_train],decoder_target_train,\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 32, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0aa85d",
   "metadata": {},
   "source": [
    "## 모델 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3da17853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 256)         1155584   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 525312    \n",
      "=================================================================\n",
      "Total params: 1,680,896\n",
      "Trainable params: 1,680,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9d0722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0197ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 256)    1859328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_2[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7263)   1866591     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,251,231\n",
      "Trainable params: 4,251,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f4ca82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcc5d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['<start>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8310d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + idx2eng[i]+' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "249a6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=fra2idx['<start>']) and i!=fra2idx['<end>']):\n",
    "            temp = temp + idx2fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b77e3d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: take it . \n",
      "정답 문장: prenez le ! \n",
      "번역기가 번역한 문장:  qu . . . . ! . ! \n",
      "-----------------------------------\n",
      "입력 문장: i have cabin fever . \n",
      "정답 문장: je me sens comme un lion en cage . \n",
      "번역기가 번역한 문장:  j me de . . . . \n",
      "-----------------------------------\n",
      "입력 문장: get the camera . \n",
      "정답 문장: prenez l appareil photo . \n",
      "번역기가 번역한 문장:  prends la . . . \n",
      "-----------------------------------\n",
      "입력 문장: i borrow money . \n",
      "정답 문장: j emprunte de l argent . \n",
      "번역기가 번역한 문장:  j ce mon de de d \n",
      "-----------------------------------\n",
      "입력 문장: anything goes here . \n",
      "정답 문장: ici tout est permis . \n",
      "번역기가 번역한 문장:  l c c regardez quell\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1,100,301,777,2222]:\n",
    "    input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', seq2src(encoder_input_test[seq_index]))\n",
    "    print('정답 문장:', seq2tar(decoder_input_test[seq_index]))\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c0ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
